import uuid
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import os
import requests
from dotenv import load_dotenv
from gigachat import GigaChat
import json
import base64



load_dotenv()
TMDB_ACCESS_TOKEN = os.getenv('TMDB_ACCESS_TOKEN')
load_dotenv()
GIGACHAT_TOKEN = os.getenv('GIGA_ACCESS_TOKEN')

if not TMDB_ACCESS_TOKEN:
    raise ValueError("TMDB_ACCESS_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")

TMDB_IMAGE_BASE_URL = "https://image.tmdb.org/t/p/w500"
# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
model = SentenceTransformer('all-MiniLM-L6-v2')

if not GIGACHAT_TOKEN:
    raise ValueError("GIGACHAT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")

def load_or_create_embeddings():
    # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
    embeddings_file = "movie_embeddings.npy"
    data_file = "movies_metadata.csv"
    
    if os.path.exists(embeddings_file) and os.path.exists(data_file):
        print("–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        embeddings = np.load(embeddings_file)
        df = pd.read_csv(data_file)
        df['embedding'] = list(embeddings)
    else:
        print("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = pd.read_csv("tmdb_5000_movies.csv")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
        df = df[['id', 'original_title', 'overview', 'genres']].dropna()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –æ–ø–∏—Å–∞–Ω–∏–π —Ñ–∏–ª—å–º–æ–≤
        print("–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ñ–∏–ª—å–º–æ–≤...")
        embeddings = df['overview'].apply(lambda x: model.encode(x, show_progress_bar=False))
        df['embedding'] = embeddings
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –¥–∞–Ω–Ω—ã—Ö
        np.save(embeddings_file, np.array(embeddings.tolist()))
        df.to_csv(data_file, index=False)
    
    return df

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
df = load_or_create_embeddings()



def generate_explanation(user_input, movie_title, movie_overview, similarity):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é GigaChat API"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω —á–µ—Ä–µ–∑ get_gigachat_token
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º GigaChat —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π
        giga = GigaChat(
            credentials=GIGACHAT_TOKEN,
            verify_ssl_certs=False,
            scope="GIGACHAT_API_PERS"
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = f"""
Write a short explanation (maximum 100 words) describing why this movie is a great match for the query: "{user_input}".
Movie description: {movie_overview}
Do not repeat the movie title or retell the plot. Instead, provide a thoughtful, conversational explanation that clearly highlights the key reasons‚Äîsuch as themes, atmosphere, or emotional tone‚Äîthat make it a strong fit. 
Imagine you're recommending it to a friend and want them to see why it matches their interest, try to use not complex language.
"""
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç GigaChat
        response = giga.chat(prompt)
        explanation = response.choices[0].message.content
        
        # –ï—Å–ª–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ, –æ–±—Ä–µ–∑–∞–µ–º –µ–≥–æ
        if len(explanation) > 10000:
            explanation = explanation[:10000] + "..."
            
        return explanation
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è: {str(e)}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –±–∞–∑–æ–≤–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
        if similarity > 0.8:
            return f"–≠—Ç–æ—Ç —Ñ–∏–ª—å–º –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–µ–º—É —Å—é–∂–µ—Ç—É –∏ —Ç–µ–º–∞—Ç–∏–∫–µ."
        elif similarity > 0.6:
            return f"–≠—Ç–æ—Ç —Ñ–∏–ª—å–º —Ö–æ—Ä–æ—à–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –∏ –º–æ–∂–µ—Ç –≤–∞—Å –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å."
        else:
            return f"–≠—Ç–æ—Ç —Ñ–∏–ª—å–º —á–∞—Å—Ç–∏—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –∏ –º–æ–∂–µ—Ç –≤–∞—Å —É–¥–∏–≤–∏—Ç—å."


def recommend_movies(user_input, top_k=9):
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —Ñ–∏–ª—å–º–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞"""
    print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–≤–æ–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è...")
    user_embedding = model.encode(user_input)
    similarities = df['embedding'].apply(lambda x: cosine_similarity([user_embedding], [x])[0][0])
    df['similarity'] = similarities
    recommendations = df.sort_values('similarity', ascending=False).head(top_k)
    
    # –î–æ–±–∞–≤–ª—è–µ–º URL –ø–æ—Å—Ç–µ—Ä–æ–≤ –∏ –≥–æ–¥ –≤—ã–ø—É—Å–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∏–ª—å–º–∞
    movie_data = recommendations['id'].apply(get_movie_poster)
    recommendations['poster_path'] = movie_data.apply(lambda x: x['poster_path'])
    recommendations['release_year'] = movie_data.apply(lambda x: x['release_year'])
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∏–ª—å–º–∞
    recommendations['explanation'] = recommendations.apply(
        lambda row: generate_explanation(user_input, row['original_title'], row['overview'], row['similarity']),
        axis=1
    )
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Å–æ–ª—å
    print("\nüé¨ –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–ª—å–º—ã:")
    for i, row in recommendations.iterrows():
        print(f"\nüé• {row['original_title']} ({row['release_year']})")
        print(f"üìù {row['explanation']}")
        print(f"üìã –ñ–∞–Ω—Ä—ã: {row['genres']}")
        print(f"üîó –ü–æ—Å—Ç–µ—Ä: {row['poster_path']}")
        print("-" * 80)
    
    return recommendations[['id', 'original_title', 'overview', 'genres', 'release_year', 'poster_path', 'explanation']]

def get_movie_poster(movie_id):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å—Ç–µ—Ä–∞ —Ñ–∏–ª—å–º–∞ –∏ –≥–æ–¥–∞ –≤—ã–ø—É—Å–∫–∞ –∏–∑ TMDB –ø–æ ID"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∏–ª—å–º–µ –ø–æ ID
        movie_url = f"https://api.themoviedb.org/3/movie/{movie_id}"
        headers = {
            "Authorization": f"Bearer {TMDB_ACCESS_TOKEN}",
            "accept": "application/json"
        }
        
        response = requests.get(movie_url, headers=headers)
        response.raise_for_status()
        movie_data = response.json()
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –ø–æ—Å—Ç–µ—Ä—É –∏ –≥–æ–¥ –≤—ã–ø—É—Å–∫–∞
        poster_url = "https://via.placeholder.com/500x750?text=No+Poster+Available"
        if movie_data.get("poster_path"):
            poster_url = f"{TMDB_IMAGE_BASE_URL}{movie_data['poster_path']}"
            
        # –ü–æ–ª—É—á–∞–µ–º –≥–æ–¥ –∏–∑ release_date
        release_year = None
        if movie_data.get("release_date"):
            release_year = movie_data["release_date"][:4]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 4 —Å–∏–º–≤–æ–ª–∞ (–≥–æ–¥)
            
        return {
            "poster_path": poster_url,
            "release_year": release_year
        }
        
    except requests.exceptions.RequestException as e:
        print(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∏–ª—å–º–∞ {movie_id}: {str(e)}")
        return {
            "poster_path": "https://via.placeholder.com/500x750?text=Network+Error",
            "release_year": None
        }
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∏–ª—å–º–∞ {movie_id}: {str(e)}")
        return {
            "poster_url": "https://via.placeholder.com/500x750?text=Error+Loading+Poster",
            "release_year": None
        }

# –¢–µ—Å—Ç–æ–≤—ã–π –∫–æ–¥
if __name__ == "__main__":
    user_mood = "I want to watch a movie about a girl who is a detective"
    recommendations = recommend_movies(user_mood)
    print("\nüé¨ –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–ª—å–º—ã:")
    for i, row in recommendations.iterrows():
        print(f"\nüé• {row['original_title']}\n{row['overview']}\n{row['explanation']}")